<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Sebjin • Projects • Adversarial Training</title>
  <link rel="stylesheet" href="../../style.css" />
  <link href="https://fonts.googleapis.com/css2?family=Golos+Text:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="titlename">
        <nav class="title-breadcrumbs" aria-label="Breadcrumb">
            <ol class="crumbs">
                <li><a class="link-title-btn" href="../../index.html">sebjin</a></li>
                <li><a class="link-title-btn" href="../../index.html">made</a></li>
                <li><a class="link-title-btn" href="../projects.html">projects</a></li>
                <li><a class="link-title-btn" href="adversarialtraining.html">adversarial training</a></li>
            </ol>
        </nav>
    </div>
    <main>
        <!-- GitHub link -->
        <p>
        <a class="link-btn" href="https://github.com/sebjinK/adversarial-training" target="_blank" rel="noopener">
            View on GitHub
        </a>
        </p>
        <!-- Blog post content -->
        <article class="blog-post">
        <p>
            <!-- Replace this with your description -->
            Adversarial training is a defensive technique in machine learning where models are trained
            not only on clean data, but also on adversarially perturbed examples. By exposing the model 
            to attacks during training, it can build resilience against them. This project demonstrates 
            implementations using MNIST, FGSM, and PGD attacks.
        </p>
    </main>
</body>
</html>